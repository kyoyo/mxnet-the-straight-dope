{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with ``gluon``\n",
    "\n",
    "Now that we've implemented a whole neural network from scratch, using nothing but ``mx.ndarray`` and ``mxnet.autograd``, let's see how we can make the same model while doing a lot less work. \n",
    "\n",
    "Again, let's import some packages, this time adding ``mxnet.gluon`` to the list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context\n",
    "\n",
    "We'll also want to set a context to tell gluon where to do most of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "\n",
    "Again we'll look at the problem of linear regression and stick with the same synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "    \n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "noise = 0.01 * nd.random_normal(shape=(num_examples,))\n",
    "y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data iterator\n",
    "\n",
    "We'll stick with the ``DataLoader`` for handling out data batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X, y),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "When we implemented things from scratch, \n",
    "we had to individually allocate parameters \n",
    "and then compose them together as a model. \n",
    "While it's good to know how to do things from scratch, \n",
    "with `gluon`, we can just compose a network from predefined layers. \n",
    "For a linear model, the appropriate layer is called `Dense`. \n",
    "It's called a *dense* layer because every node in the input \n",
    "is connected to every node in the subsequent layer. \n",
    "That description seems excessive \n",
    "because we only have one (non-input) layer here, \n",
    "and that layer only contains one node!\n",
    "But in subsequent chapters we'll typically work \n",
    "with networks that have multiple outputs, \n",
    "so we might as well start thinking in terms of layers of nodes. \n",
    "Because a linear model consists of just a single `Dense` layer, we can instantiate it with one line.\n",
    "\n",
    "但在后面的章节中，我们将通常处理具有多个输出的网络，因此我们不妨从节点层开始考虑。因为线性模型只包含一个致密层，我们可以用一条线来实例化它。\n",
    "\n",
    "As in [the previous notebook](linear-regression-scratch.ipynb), \n",
    "we have an inputdimension of 2 and an output dimension of 1. \n",
    "the most direct way to instantiate a ``Dense`` layer with these dimensions\n",
    "is to specify the number of inputs and the number of outputs. \n",
    "\n",
    "在以前的notebook，我们有2个输入维度和1输出维度。用这些维度来实例化稠密层的最直接的方法是指定输入的数量和输出的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(1, in_units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've already got a neural network. \n",
    "Like our hand-crafted model in the previous notebook, \n",
    "this model has a weight matrix and bias vector.\n",
    "\n",
    "我们已经有了一个神经网络。就像我们在前面notebook里手工制作的模型一样，这个模型有一个权重矩阵和偏置向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dense0_weight (shape=(1, 2), dtype=<class 'numpy.float32'>)\n",
      "Parameter dense0_bias (shape=(1,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "print(net.weight)\n",
    "print(net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `net.weight` and `net.bias` are not actually NDArrays.\n",
    "They are instances of the `Parameter` class.\n",
    "We use `Parameter` instead of directly accessing NDAarrays for several reasons. \n",
    "For example, they provide convenient abstractions for initializing values.\n",
    "Unlike NDArrays, Parameters can be associated with multiple contexts simulataneously.\n",
    "This will come in handy in future chapters when we start thinking about distributed learning across multiple GPUs.\n",
    "\n",
    "在这里，net.weight和net.bias实际上并不是ndarrays。它们的`Parameter`的实例。我们使用`Parameter`，而不是直接访问ndaarrays有几个原因。例如，它们为初始化值提供了方便的抽象。不像ndarrays，参数可以同时关联到多个contexts。在今后我们开始考虑跨多个GPU的分布式学习的章节里,这将派上用场.\n",
    "\n",
    "In `gluon`, all neural networks are made out of Blocks (`gluon.Block`).\n",
    "Blocks are just units that take inputs and generates outputs.\n",
    "Blocks also contain parameters that we can update. \n",
    "Here, our network consists of only one layer, \n",
    "so it's convenient to access our parameters directly. \n",
    "When our networks consist of 10s of layers, this won't be so fun.\n",
    "No matter how complex our network, \n",
    "we can grab all its parameters by calling `collect_params()` as follows:\n",
    "\n",
    "在 `gluon`中，所有的神经网络都是由块(`gluon.Block`）组成的。块只是输入和产生输出的单元。块还包含可以更新的参数。这里，我们的网络只有一层，所以直接访问我们的参数是很方便的。当我们的网络是由几十层，就不会那么简单。无论我们的网络多么复杂，我们可以通过调用 `collect_params()` 抓住所有的参数，如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense0_ (\n",
       "  Parameter dense0_weight (shape=(1, 2), dtype=<class 'numpy.float32'>)\n",
       "  Parameter dense0_bias (shape=(1,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object is a `gluon.parameter.ParameterDict`. \n",
    "This is a convenient abstaction for retrieving and manipulating groups of Parameter objects.\n",
    "Most often, we'll want to retrieve all of the parameters in a neural network \n",
    "\n",
    "返回的对象是一个 `gluon.parameter.ParameterDict`。用于检索和操作参数对象群，这是一个方便的抽象。大多数情况下，我们希望获得神经网络中的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.gluon.parameter.ParameterDict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters\n",
    "Once we initialize our Parameters, we can access their underlying data and context(s),\n",
    "and we can also feed data through the neural network to generate output.\n",
    "However, we can't get going just yet. \n",
    "If we try invoking your model by calling ``net(nd.array([[0,1]]))``, \n",
    "we'll confront the following hideous error message:\n",
    "\n",
    "一旦初始化参数，我们就可以访问它们的底层数据和上下文，我们也可以通过神经网络来输入数据以产生输出。然而，我们现在还不能去做。\n",
    "如果我们尝试调用你的模型通过调用``net(nd.array([[0,1]]))``, \n",
    "我们将面对以下可怕的错误消息：\n",
    "\n",
    "``RuntimeError: Parameter dense1_weight has not been initialized. \n",
    "Note that you should initialize parameters and create Trainer \n",
    "with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks``.\n",
    "\n",
    "That's because we haven't yet told ``gluon`` what the *initial values* for our parameters should be!\n",
    "We initialize parameters by calling the `.initialize()` method of a Parameterdict. \n",
    "We'll need to pass in two arguments. \n",
    "\n",
    "这是因为我们还没有告诉 ``gluon`` 我们的参数的*初始值*应该是多少！我们通过调用一个Parameterdict `.initialize()` 方法来初始化参数。我们需要传递两个参数。\n",
    "\n",
    "* An initializer, many of which live in the `mx.init` module. \n",
    "* A context where the parameters should live. In this case we'll pass in the `model_ctx`. Most often this will either be a GPU or a list of GPUs. \n",
    "\n",
    "\n",
    "\n",
    "* 初始值设定项，其中许多都存在于`mx.init`模块中。\n",
    "* 包含参数上下文。在这种情况下，我们会传入` model_ctx `。这通常是一个或一系列GPU。\n",
    " \n",
    " \n",
    "*MXNet* provides a variety of common initializers in ``mxnet.init``.\n",
    "To keep things consistent with the model we built by hand, \n",
    "we'll initialize each parameter by sampling from a standard normal distribution, \n",
    "using `mx.init.Normal(sigma=1.)`.\n",
    "\n",
    "*MXNet*  在``mxnet.init``包里提供各种常见初始化器 \n",
    "为了使事情与我们手工构建的模型保持一致，我们将使用标准的正态分布抽样，对每个参数进行初始化，使用 `mx.init.Normal(sigma=1.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deferred Initialization\n",
    "\n",
    "When we call ``initialize``, ``gluon`` associates each parameter with an initializer.\n",
    "However, the *actual initialization* is deferred until we make a first forward pass. \n",
    "In other words, the parameters are only initialized when they're needed. \n",
    "If we try to call `net.weight.data()` we'll get the following error:\n",
    "\n",
    "## 延迟初始化\n",
    "当我们调用 ``initialize``, ``gluon`` 将每个参数与初始化器相关联。但是，*实际初始化*被推迟到我们取得第一个前向传递的时候。换句话说，参数只有在需要时才初始化。如果我们试着调用 `net.weight.data()`  我们会得到下面的错误：\n",
    "\n",
    "``DeferredInitializationError: Parameter dense2_weight has not been initialized yet because initialization was deferred. Actual initialization happens during the first forward pass. Please pass one batch of data through the network before accessing Parameters.``\n",
    "\n",
    "Passing data through a `gluon` model is easy. \n",
    "We just sample a batch of the appropriate shape and call `net` \n",
    "just as if it were a function. \n",
    "This will invoke net's `forward()` method.\n",
    "\n",
    "通过`gluon`模型传递数据是很容易的。我们只是对一组合适的形状进行采样，并调用`net` 好像它是一个函数一样。这将调用net的forward()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 8.98771858]]\n",
       "<NDArray 1x1 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = nd.array([[4,7]])\n",
    "net(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `net` is initialized, we can access each of its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.85240513  0.79687119]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "\n",
      "[ 0.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(net.weight.data())\n",
    "print(net.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape inference\n",
    "\n",
    "Recall that previously, we instantiated our network with `gluon.nn.Dense(1, in_units=2)`. \n",
    "One slick feature that we can take advantage of in ``gluon`` is shape inference on parameters. \n",
    "Because our parameters never come into action until we pass data through the network,\n",
    "we don't actually have to declare the input dimension (`in_units`). \n",
    "Let's try this again, but letting `gluon` do more of the work:\n",
    "\n",
    "# 形状推断\n",
    "\n",
    "回想以前，我们通过 `gluon.nn.Dense(1, in_units=2)` 实例化我们的网络。我们可以利用``gluon``的一个聪明的特性，那就是参数的形状推断。因为我们传递数据到网络之前我们的参数从来不会被初始化，我们实际上不必申明输入维度（in_units）。让我们再试一次，但会让`gluon` 做更多的工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll elaborate on this and more of ``gluon``'s internal workings in subsequent chapters.\n",
    "\n",
    "我们将在后面的章节中详细阐述这个和更多的``gluon``的内部运作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss\n",
    "\n",
    "Instead of writing our own loss function we're just going to access squared error by instantiating ``gluon.loss.L2Loss``. Just like layers, and whole networks, a loss in gluon is just a `Block`.\n",
    "\n",
    "## 定义损失\n",
    "\n",
    "我们将仅仅通过实例化 ``gluon.loss.L2Loss`` 来获取均方误差，而不是写我们自己的损失函数 。就像层和整个网络一样，损失在gluon中只是一个“块”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Instead of writing stochastic gradient descent from scratch every time, we can instantiate a ``gluon.Trainer``, passing it a dictionary of parameters. Note that the ``sgd`` optimizer in ``gluon`` actually uses SGD with momentum and clipping (both can be switched off if needed), since these modifications make it converge rather much better. We will discuss this later when we go over a range of optimization algorithms in detail.\n",
    "\n",
    "## 优化器\n",
    "\n",
    "我们不必每次都从头开始写随机梯度下降，我们可以实例化一个``gluon.Trainer``，将一个字典型参数传给它。注意在``gluon``中， ``sgd``优化器实际使用SGD动量和裁剪（都可以关闭，如果需要的话），因为这些修改使其更好收敛。稍后我们将详细讨论一系列优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop\n",
    "\n",
    "You might have noticed that it was a bit more concise to express our model in ``gluon``. For example, we didn't have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. The benefits of relying on ``gluon``'s abstractions will grow substantially once we start working with much more complex models. But once we have all the basic pieces in place, the training loop itself is quite similar to what we would do if implementing everything from scratch. \n",
    "\n",
    "你可能已经注意到，用``gluon``表达我们的模型是比较简洁的。例如，我们不需要单独分配参数，定义损失函数，或者实现随机梯度下降。一旦我们开始使用更复杂的模型，依赖``gluon`` 的抽象的好处将大大增加。但是一旦我们有了所有基本的部分，训练循环本身就非常类似于我们从头开始实现一切。\n",
    "\n",
    "\n",
    "## 训练\n",
    "\n",
    "To refresh your memory. For some number of ``epochs``, we'll make a complete pass over the dataset (``train_data``), grabbing one mini-batch of inputs and the corresponding ground-truth labels at a time. \n",
    "\n",
    "刷新一下你的记忆。对于一些` `epochs` `，我们会做一个完整的传过来的数据集（` ` train_data ` `），每次抓取输入里的一个小batch和相应的真实标签。\n",
    "\n",
    "Then, for each batch, we'll go through the following ritual. So that this process becomes maximally ritualistic, we'll repeat it verbatim:\n",
    "然后，每一个batch，我们将通过一下惯例。让这个过程变得最大限度的惯例，我们将逐字地重复：\n",
    "\n",
    "* Generate predictions (``yhat``) and the loss (``loss``) by executing a forward pass through the network.\n",
    "* Calculate gradients by making a backwards pass through the network via ``loss.backward()``. \n",
    "* Update the model parameters by invoking our SGD optimizer (note that we need not tell ``trainer.step`` about which parameters but rather just the amount of data, since we already performed that in the initialization of ``trainer``).\n",
    "\n",
    "\n",
    "* 通过网络执行一个forward pass 前向传递 生成预测 (``yhat``) 和损失(``loss``) \n",
    "* 利用``loss.backward()`` 通过网络生成一个反向传递来计算梯度。\n",
    "* 通过调用我们的SGD优化器来更新模型参数（注意，我们不需要告诉``trainer.step``哪些参数，而只是数据量，因为我们在``trainer``初始化中已经完成这些）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.0107051503119\n",
      "Epoch 1, loss: 1.30396030197e-05\n",
      "Epoch 2, loss: 1.30381947897e-05\n",
      "Epoch 3, loss: 1.3109341069e-05\n",
      "Epoch 4, loss: 1.31099881296e-05\n",
      "Epoch 5, loss: 1.30834395102e-05\n",
      "Epoch 6, loss: 1.30859975455e-05\n",
      "Epoch 7, loss: 1.30581023294e-05\n",
      "Epoch 8, loss: 1.30572632249e-05\n",
      "Epoch 9, loss: 1.30876914578e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = .0001\n",
    "loss_sequence = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    # inner loop\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.mean(loss).asscalar()\n",
    "    print(\"Epoch %s, loss: %s\" % (e, cumulative_loss / num_examples))\n",
    "    loss_sequence.append(cumulative_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learning curve\n",
    "Now let's check how quickly SGD learns the linear regression model by plotting the learning curve.\n",
    "\n",
    "\n",
    "## 可视化学习曲线\n",
    "现在让我们通过绘制学习曲线，来看看SGD如何快速地学习线性回归模型的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'average loss')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAF7CAYAAAA35zlzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvKaGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe0cfd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the convergence of the estimated loss function \n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(loss_sequence)\n",
    "\n",
    "# Adding some bells and whistles to the plot\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.xlabel('epoch',fontsize=14)\n",
    "plt.ylabel('average loss',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss function converges quickly to the optimal solution.\n",
    "\n",
    "可以看出，损失函数收敛到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the learned model parameters\n",
    "\n",
    "As an additional sanity check, since we generated the data from a Gaussian linear regression model, we want to make sure that the learner managed to recover the model parameters, which were set to weight $2,-3.4$ with an offset of $4.2$.\n",
    "\n",
    "\n",
    "## 获取学习模型参数\n",
    "作为一个额外的健全检查，由于我们从高斯线性回归模型中生成数据，我们希望确保学习者设法找到模型参数，这些参数被设置为2，3.4，偏移量为4.2。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of \"params\" is a  <class 'mxnet.gluon.parameter.ParameterDict'>\n",
      "dense1_weight \n",
      "[[ 1.99950063 -3.40032887]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "dense1_bias \n",
      "[ 4.1995225]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params() # this returns a ParameterDict\n",
    "\n",
    "print('The type of \"params\" is a ',type(params))\n",
    "\n",
    "# A ParameterDict is a dictionary of Parameter class objects\n",
    "# therefore, here is how we can read off the parameters from it.\n",
    "\n",
    "for param in params.values():\n",
    "    print(param.name,param.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "As you can see, even for a simple example like linear regression, ``gluon`` can help you to write quick, clean, code. Next, we'll repeat this exercise for multi-layer perceptrons, extending these lessons to deep neural networks and (comparatively) real datasets. \n",
    "\n",
    "## 结论\n",
    "\n",
    "正如你所看到的，即使是像线性回归这样的简单例子，“``gluon`` 也能帮助你写出快速、干净的代码。接下来，我们将对多层感知器重复这个练习，将这些教程扩展到深度神经网络和真实数据集（相对的）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[The perceptron algorithm](../chapter02_supervised-learning/perceptron.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
