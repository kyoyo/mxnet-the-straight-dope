{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "So far we did not worry very much about where the data came from and how the models that we build get deployed. Not caring about it can be problematic. Many failed machine learning deployments can be traced back to this situation. This chapter is meant to help with detecting such situations early and points out how to mitigate them. Depending on the case this might be rather simple (ask for the 'right' data) or really difficult (implement a reinforcement learning system). \n",
    "\n",
    "到目前为止，我们并不十分关心数据来自何处以及我们构建的模型如何部署。 不关心它可能是有问题的。 许多失败的机器学习部署可以追溯到这种情况。 本章旨在帮助及早发现这些情况，并指出如何缓解这些情况。 根据情况，这可能相当简单（要求“正确的”数据）或者真的很困难（实施强化学习系统）。\n",
    "\n",
    "## Covariate Shift\n",
    "\n",
    "At its heart is a problem that is easy to understand but also equally easy to miss. Consider being given the challenge of distinguishing cats and dogs. Our training data consists of images of the following kind:\n",
    "\n",
    "其核心是一个容易理解的问题，同样容易弄错。 考虑给予区分猫和狗的挑战。 我们的训练数据由以下类型的图像组成：\n",
    "\n",
    "|![](../img/cat1.jpg)|![](../img/cat2.jpg)|![](../img/dog1.jpg)|![](../img/dog2.jpg)|\n",
    "|:---------------:|:---------------:|:---------------:|:---------------:|\n",
    "|cat|cat|dog|dog|\n",
    "\n",
    "At test time we are asked to classify the following images:\n",
    "\n",
    "|![](../img/cat-cartoon1.png)|![](../img/cat-cartoon2.png)|![](../img/dog-cartoon1.png)|![](../img/dog-cartoon2.jpg)|\n",
    "|:---------------:|:---------------:|:---------------:|:---------------:|\n",
    "|cat|cat|dog|dog|\n",
    "\n",
    "Obviously this is unlikely to work well. The training set consists of photos, while the test set contains only cartoons. The colors aren't even accurate. Training on a dataset that looks substantially different from the test set without some plan for how to adapt to the new domain is a bad idea. Unfortunately, this is a very common pitfall. Statisticians call this **Covariate Shift**, i.e. the situation where the distribution over the covariates (aka training data) is shifted on test data relative to the training case. Mathematically speaking, we are referring the case where $p(x)$ changes but $p(y|x)$ remains unchanged.\n",
    "\n",
    "显然这是不太可能的。 训练集由照片组成，而测试集只包含漫画。 颜色甚至不准确。 对一个看起来与测试集大不相同的数据集进行培训，而没有一些如何适应新的领域的计划是一个坏主意。 不幸的是，这是一个非常常见的陷阱。 统计学家将这种协变量称为协变量（covariate Shift），即协变量（又称训练数据）上的分布相对于训练案例而言在测试数据上的偏移。 在数学上，我们指的是p（x）改变但p（y | x）保持不变的情况。\n",
    "\n",
    "## Concept Shift\n",
    "\n",
    "A related problem is that of concept shift. This is the situation where the the labels change. This sounds weird - after all, a cat is a cat is a cat. Well, cats maybe but not soft drinks. There is considerable concept shift throughout the USA, even for such a simple term:\n",
    "\n",
    "一个相关的问题是概念转换。 这是标签改变的情况。 这听起来很奇怪 - 毕竟，猫是猫是猫。 那么，猫可能但不是软饮料。 即使在这样一个简单的术语中，整个美国也有相当多的概念转移：\n",
    "\n",
    "![](../img/sodapopcoke.png)\n",
    "\n",
    "If we were to build a machine translation system, the distribution $p(y|x)$ would be different, e.g. depending on our location. This problem can be quite tricky to spot. A saving grace is that quite often the $p(y|x)$ only shifts gradually (e.g. the click-through rate for NOKIA phone ads). Before we go into further details, let us discuss a number of situations where covariate and concept shift are not quite as blatantly obvious.\n",
    "\n",
    "\n",
    "如果我们要建立机器翻译系统，分布 $p(y|x)$ 将是不同的，例如， 取决于我们的位置。 这个问题很难找到。 节约的优惠是，$p(y|x)$ $只是逐渐变化（例如NOKIA手机广告的点击率）。 在进一步详细讨论之前，让我们讨论一些协变量和概念转换并不十分明显的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Medical Diagnostics\n",
    "\n",
    "Imagine you want to design some algorithm to detect cancer. You get data of healthy and sick people; you train your algorithm; it works fine, giving you high accuracy and you conclude that you’re ready for a successful career in medical diagnostics. Not so fast ...\n",
    "\n",
    "想象一下，你想设计一些算法来检测癌症。 你得到健康和有病的人的数据; 你训练你的算法; 它工作正常，给你高准确性，你的结论是你已经准备好在医疗诊断成功的职业生涯。 没那么快\n",
    "\n",
    "Many things could go wrong. In particular, the distributions that you work with for training and those in the wild might differ considerably. This happened to an unfortunate startup I had the opportunity to consult for many years ago. They were developing a blood test for a disease that affects mainly older men and they’d managed to obtain a fair amount of blood samples from patients. It is considerably more difficult, though, to obtain blood samples from healthy men (mainly for ethical reasons). To compensate for that, they asked a large number of students on campus to donate blood and they performed their test. Then they asked me whether I could help them build a classifier to detect the disease. I told them that it would be very easy to distinguish between both datasets with probably near perfect accuracy. After all, the test subjects differed in age, hormone level, physical activity, diet, alcohol consumption, and many more factors unrelated to the disease. This was unlikely to be the case with real patients: Their sampling procedure had caused an extreme case of covariate shift that couldn’t be corrected by conventional means. In other words, training and test data were so different that nothing useful could be done and they had wasted significant amounts of money. \n",
    "\n",
    "\n",
    "很多事情可能会出错。特别是，你在训练中的分布和训练之外的分布可能有很大的不同。这发生在一个不幸的初创公司，我有机会咨询了很多年前。他们正在对一种主要影响老年人的疾病进行血液检测，并设法从患者身上获得相当数量的血样。然而，从健康人（主要出于伦理原因）获取血液样本要困难得多。为了弥补这一点，他们要求校园里大量的学生献血，并进行了测试。然后他们问我是否可以帮助他们建立一个分类器来检测这种疾病。我告诉他们，很容易区分这两个数据集，它们可能接近完美的精度。毕竟，受试者的年龄、激素水平、体力活动、饮食、饮酒以及与疾病无关的许多因素各不相同。这种情况不太可能发生在真实的病人身上：他们的取样过程导致了一个极端的情况，不能用常规方法纠正协变量的转移。换言之，训练和测试数据是如此不同，没有什么有用的可以做，他们浪费了大量的钱。\n",
    "\n",
    "### Self Driving Cars\n",
    "\n",
    "A company wanted to build a machine learning system for self-driving cars. One of the key components is a roadside detector. Since real annotated data is expensive to get, they had the (smart and questionable) idea to use synthetic data from a game rendering engine as additional training data. This worked really well on 'test data' drawn from the rendering engine. Alas, inside a real car it was a disaster. As it turned out, the roadside had been rendered with a very simplistic texture. More importantly, *all* the roadside had been rendered with the *same* texture and the roadside detector learned about this 'feature' very quickly. \n",
    "\n",
    "一家公司想要建立一个自驾车的机器学习系统。其中一个关键部件是路边探测器。由于真正的注释数据是昂贵的，他们有（聪明的和有疑问的）想法使用合成数据从游戏渲染引擎作为额外的训练数据。这对从渲染引擎绘制的测试数据非常有效。唉，在一辆真正的车里，真是一场灾难。结果，路边呈现出一种非常简单的质感。更重要的是，所有的路边都呈现出同样的纹理，路边探测器很快就了解到了这个“特征”。\n",
    "\n",
    "A similar thing happened to the US Army when they first tried to detect tanks in the forest. They took aerial photographs of the forest without tanks, then drove the tanks into the forest and took another set of pictures. The so-trained classifier worked 'perfectly'. Unfortunately, all it had learned was to distinguish trees with shadows from trees without shadows - the first set of pictures was taken in the early morning, the second one at noon. \n",
    "\n",
    "当美军第一次探测到森林中的坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。训练有素的分类器工作得很好。不幸的是，它所学到的只是区分树木和阴影，而没有阴影的树木——第一组照片是在清晨拍摄的，第二组是在中午拍摄的。\n",
    "\n",
    "### Nonstationary distributions\n",
    "\n",
    "A much more subtle situation is where the distribution changes slowly and the model is not updated adequately. Here are a number of typical cases:\n",
    "\n",
    "* We train a computational advertising model and then fail to update it frequently (e.g. we forget to incorporate that an obscure new device called an iPad was just launched). \n",
    "* We build a spam filter. It works well at detecting all spam that we've seen so far. But then the spammers wisen up and craft new messages that look quite unlike anything we've seen before. \n",
    "* We build a product recommendation system. It works well for the winter. But then it keeps on recommending Santa hats after Christmas.\n",
    "\n",
    "\n",
    "### 非平稳分布\n",
    "\n",
    "\n",
    "一个更加微妙的情况是，分布变化缓慢，模型没有充分更新。下面是一些典型的例子：\n",
    "\n",
    "* 我们训练一个计算广告模型，然后不能经常更新它（例如我们忘记合并一个叫做iPad的不知名的新设备）。\n",
    "* 我们构建一个垃圾邮件过滤器。它能很好地检测到我们迄今为止看到的所有垃圾邮件。但是垃圾邮件发送者的聪明和构思新的消息，看起来完全不像之前我们所看到的。\n",
    "* 建立产品推荐系统。冬天的效果很好。但圣诞节过后，它一直在推荐圣诞老人的帽子。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### More Anecdotes\n",
    "\n",
    "* We build a classifier for \"Not suitable/safe for work\" (NSFW) images. To make our life easy, we scrape a few seedy Subreddits. Unfortunately the accuracy on real life data is lacking (the pictures posted on Reddit are mostly 'remarkable' in some way, e.g. being taken by skilled photographers, whereas most real NSFW images are fairly unremarkable ...). Quite unsurprisingly the accuracy is not very high on real data.\n",
    "* We build a face detector. It works well on all benchmarks. Unfortunately it fails on test data - the offending examples are close-ups where the face fills the entire image (no such data was in the training set). \n",
    "* We build a web search engine for the USA market and want to deploy it in the UK. \n",
    "\n",
    "\n",
    "### 更多的轶事\n",
    "\n",
    "我们建立一个“工作”不适合/安全分类（NSFW）图像。让我们的生活方便，我们把一些破旧的subreddits。不幸的是，现实生活中的数据的准确性缺乏（在Reddit上发布的照片大多是“显着”以某种方式，例如由熟练的摄影师，而最真实的NSFW形象相当不起眼…）。毫不奇怪，真实数据的准确率并不高。\n",
    "* 我们建立一个面部探测器。它在所有基准测试中运行良好。不幸的是，它在测试数据上失败了——违规的例子是脸部填充整个图像的特写镜头（在训练集中没有这样的数据）。\n",
    "* 我们为美国市场建立了一个网络搜索引擎，并希望将其部署到英国。\n",
    "\n",
    "In short, there are many cases where training and test distribution $p(x)$ are different. In some cases, we get lucky and the models work despite the covariate shift. We now discuss principled solution strategies. Warning - this will require some math and statistics.\n",
    "\n",
    "简言之，训练和测试分布$p(x)$的情况是不同的。在某些情况下，我们很幸运，尽管协变变换，模型仍能工作。我们现在讨论原则性解决策略。警告-这需要一些数学和统计学。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariate Shift Correction\n",
    "\n",
    "Assume that we want to estimate some dependency $p(y|x)$ for which we have labeled data $(x_i,y_i)$. Alas, the observations $x_i$ are drawn from some distribution $q(x)$ rather than the ‘proper’ distribution $p(x)$. To make progress, we need to reflect about what exactly is happening during training: we iterate over training data and associated labels $\\{(x_1, y_1), \\ldots (y_m, y_m)\\}$ and update the weight vectors of the model after every minibatch. Depending on the situation we also apply some penalty to the parameters, e.g. $L_2$ regularization. In other words, we want to solve\n",
    "\n",
    "# # 协变量校正\n",
    "\n",
    "假设我们要估计一些依赖$p(y|x)$ 的我们有标记数据$(x_i,y_i)$。唉，观察$x_i$从一些分布q（x）为画而不是“合适”的分布P（x）$。为了取得进展，我们需要思考，究竟是发生在训练：我们重申了训练数据和相关的标签$\\{(x_1, y_1), \\ldots (y_m, y_m)\\}和更新权重向量模型在每个minibatch。根据不同的情况也适用于一些惩罚参数，如l_2美元美元的正则化。换句话说，我们要解决\n",
    "\n",
    "$$\\mathop{\\mathrm{minimize}}_w \\frac{1}{m} \\sum_{i=1}^m l(x_i, y_i, f(x_i)) + \\frac{\\lambda}{2} \\|w\\|_2^2$$\n",
    "\n",
    "Statisticians call the first term an *empirical average*, that is an average computed over the data drawn from $p(x) p(y|x)$. If the data is drawn from the 'wrong' distribution $q$, we can correct for that by using the following simple identity:\n",
    "\n",
    "$$\\mathbf{E}_{x \\sim p(x)} [f(x)] = \\int f(x) p(x) dx = \\int f(x) \\frac{p(x)}{q(x)} q(x) dx = \\mathbf{E}_{x \\sim q(x)} \\left[f(x) \\frac{p(x)}{q(x)}\\right]$$\n",
    "\n",
    "In other words, we need to re-weight each instance by the ratio of probabilities that it would have been drawn from the correct distribution $\\beta(x) := p(x)/q(x)$. Alas, we do not know that ratio, so before we can do anything useful we need to estimate it. Many methods are available, e.g. some rather fancy operator theoretic ones which try to recalibrate the expectation operator directly using a minimum-norm or a maximum entropy principle. Note that for any such approach, we need samples drawn from both distributions - the 'true' $p$, e.g. by access to training data, and the one used for generating the training set $q$ (the latter is trivially available). \n",
    "\n",
    "In this case there exists a very effective approach that will give almost as good results: logistic regression. This is all that is needed to compute estimate probability ratios. We learn a classifier to distinguish between data drawn from $p(x)$ and data drawn from $q(x)$. If it is impossible to distinguish between the two distributions then it means that the associated instances are equally likely to come from either one of the two distributions. On the other hand, any instances that can be well discriminated should be significantly over/underweighted accordingly. For simplicity’s sake assume that we have an equal number of instances from both distributions, denoted by $x_i \\sim p(x)$ and $x_i′ \\sim q(x)$ respectively. Now denote by $z_i$ labels which are 1 for data drawn from $p$ and -1 for data drawn from $q$. Then the probability in a mixed dataset is given by\n",
    "\n",
    "$$p(z=1|x) = \\frac{p(x)}{p(x)+q(x)} \\text{ and hence } \\frac{p(z=1|x)}{p(z=-1|x)} = \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "Hence, if we use a logistic regression approach where $p(z=1|x)=\\frac{1}{1+\\exp(−f(x)}$ it follows (after some simple algebra) that $\\beta(x) = \\exp(f(x))$. In summary, we need to solve two problems: first one to distinguish between data drawn from both distributions, and then a reweighted minimization problem where we weigh terms by $\\beta$, e.g. via the head gradients. Here's a prototypical algorithm for that purpose:\n",
    "\n",
    "```\n",
    "CovariateShiftCorrector(X, Z)\n",
    "    X: Training dataset (without labels)\n",
    "    Z: Test dataset (without labels)\n",
    "    \n",
    "    generate training set with {(x_i, -1) ... (z_j, 1)}\n",
    "    train binary classifier using logistic regression to get function f\n",
    "    weigh data using beta_i = exp(f(x_i)) or \n",
    "                     beta_i = min(exp(f(x_i)), c)\n",
    "    use weights beta_i for training on X with labels Y\n",
    "``` \n",
    "\n",
    "**Generative Adversarial Networks** use the very idea described above to engineer a *data generator* such that it cannot be distinguished from a reference dataset. For this, we use one network, say $f$ to distinguish real and fake data and a second network $g$ that tries to fool the discriminator $f$ into accepting fake data as real. We will discuss this in much more detail later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Shift Correction\n",
    "\n",
    "Concept shift is much harder to fix in a principled manner. For instance, in a situation where suddenly the problem changes from distinguishing cats from dogs to one of distinguishing white from black animals, it will be unreasonable to assume that we can do much better than just training from scratch using the new labels. Fortunately, in practice, such extreme shifts almost never happen. Instead, what usually happens is that the task keeps on changing slowly. To make things more concrete, here are some examples:\n",
    "\n",
    "* In computational advertising, new products are launched, old products become less popular. This means that the distribution over ads and their popularity changes gradually and any click-through rate predictor needs to change gradually with it.\n",
    "* Traffic cameras lenses degrade gradually due to environmental wear, affecting image quality progressively.\n",
    "* News content changes gradually (i.e. most of the news remains unchanged but new stories appear).\n",
    "\n",
    "In such cases, we can use the same approach that we used for training networks to make them adapt to the change in the data. In other words, we use the existing network weights and simply perform a few update steps with the new data rather than training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Taxonomy of Learning Problems\n",
    "\n",
    "Armed with knowledge about how to deal with changes in $p(x)$ and in $p(y|x)$, let us consider a number of problems that we can solve using machine learning.\n",
    "\n",
    "* **Batch Learning.** Here we have access to training data and labels $\\{(x_1, y_1), \\ldots (x_n, y_n)\\}$, which we use to train a network $f(x,w)$. Later on, we deploy this network to score new data $(x,y)$ drawn from the same distribution. This is the default assumption for any of the problems that we discuss here. For instance, we might train a cat detector based on lots of pictures of cats and dogs. Once we trained it, we ship it as part of a smart catdoor computer vision system that lets only cats in. This is then installed in a customer's home and is never updated again (barring extreme circumstances). \n",
    "* **Online Learning.** Now imagine that the data $(x_i, y_i)$ arrives one sample at a time. More specifically, assume that we first observe $x_i$, then we need to come up with an estimate $f(x_i,w)$ and only once we've done this, we observe $y_i$ and with it, we receive a reward (or incur a loss), given our decision. Many real problems fall into this category. E.g. we need to predict tomorrow's stock price, this allows us to trade based on that estimate and at the end of the day we find out whether our estimate allowed us to make a profit. In other words, we have the following cycle where we are continuously improving our model given new observations. \n",
    "\n",
    "$$\n",
    "\\mathrm{model} ~ f_t \\longrightarrow \n",
    "\\mathrm{data} ~ x_t \\longrightarrow\n",
    "\\mathrm{estimate} ~ f_t(x_t) \\longrightarrow\n",
    "\\mathrm{observation} ~ y_t \\longrightarrow \n",
    "\\mathrm{loss} ~ l(y_t, f_t(x_t)) \\longrightarrow\n",
    "\\mathrm{model} ~ f_{t+1}\n",
    "$$\n",
    "\n",
    "* **Bandits.** They are a *special case* of the problem above. While in most learning problems we have a continuously parametrized function $f$ where we want to learn its parameters (e.g. a deep network), in a bandit problem we only have a finite number of arms that we can pull (i.e. a finite number of actions that we can take). It is not very surprising that for this simpler problem stronger theoretical guarantees in terms of optimality can be obtained. We list it mainly since this problem is often (confusingly) treated as if it were a distinct learning setting. \n",
    "* **Control (and nonadversarial Reinforcement Learning).** In many cases the environment remembers what we did. Not necessarily in an adversarial manner but it'll just remember and the response will depend on what happened before. E.g. a coffee boiler controller will observe different temperatures depending on whether it was heating the boiler previously. PID (proportional integral derivative) controller algorithms are a [popular choice](http://pidkits.com/alexiakit.html) there. Likewise, a user's behavior on a news site will depend on what we showed him previously (e.g. he will read most news only once). Many such algorithms form a model of the environment in which they act such as to make their decisions appear less random (i.e. to reduce variance).\n",
    "* **Reinforcement Learning.** In the more general case of an environment with memory, we may encounter situations where the environment is trying to *cooperate* with us (cooperative games, in particular for non-zero-sum games), or others where the environment will try to *win*. Chess, Go, Backgammon or StarCraft are some of the cases. Likewise, we might want to build a good controller for autonomous cars. The other cars are likely to respond to the autonomous car's driving style in nontrivial ways, e.g. trying to avoid it, trying to cause an accident, trying to cooperate with it, etc.\n",
    "\n",
    "One key distinction between the different situations above is that the same strategy that might have worked throughout in the case of a stationary environment, might not work throughout when the environment can adapt. For instance, an arbitrage opportunity discovered by a trader is likely to disappear once he starts exploiting it. The speed and manner at which the environment changes determines to a large extent the type of algorithms that we can bring to bear. For instance, if we *know* that things may only change slowly, we can force any estimate to change only slowly, too. If we know that the environment might change instantaneously, but only very infrequently, we can make allowances for that. These types of knowledge are crucial for the aspiring data scientist to deal with concept shift, i.e. when the problem that he is trying to solve changes over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
