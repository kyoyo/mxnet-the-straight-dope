{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression from scratch\n",
    "\n",
    "If you've made it through our tutorial on linear regression from scratch, then you're past the hardest part. You already know how to load and manipulate data, build computation graphs on the fly, and take derivatives. You also know how to define a loss function, construct a model, and write your own optimizer. \n",
    "\n",
    "Nearly all neural networks that we'll build in the real world consist of these same fundamental parts. The main differences will be the type and scale of the data, and the complexity of the models. And every year or two, a new hipster optimizer comes around, but at their core they're all subtle variations of stochastic gradient descent.\n",
    "\n",
    "So let's work on a more interesting problem now. We're going to classify images of handwritten digits like these:\n",
    "![png](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/mnist.png) \n",
    "We're going to implement a model called multiclass logistic regression. Other common names for this model include softmax regression and multinomial regression. To start, let's import our bag of libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多类逻辑回归从0开始\n",
    "\n",
    "如果你已经读过我们的教程线性回归从零开始，那么你已经过去最困难的部分。您已经知道如何加载和操作数据，快速构建计算图，并派生出派生词。您还知道如何定义损失函数、构造模型和编写自己的优化器。\n",
    "\n",
    "几乎所有我们在现实世界中构建的神经网络都是由这些相同的基本部分组成的。主要的区别是数据的类型和规模，以及模型的复杂性。每一年或两年，一个新的时髦的优化器来到我们周围，但他们的核心是随机梯度下降所有微妙的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to set the compute context for our modeling. Feel free to go ahead and change this to mx.gpu(0) if you're running on an appropriately endowed machine.\n",
    "\n",
    "我们还希望为我们的建模设置计算上下文。如果您运行在一台适当的机器上，请随意更改为 mx.gpu(0)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "This time we're going to work with real data, each a 28 by 28 centrally cropped black & white photograph of a handwritten digit. Our task will be come up with a model that can associate each image with the digit (0-9) that it depicts.\n",
    "\n",
    "To start, we'll use MXNet's utility for grabbing a copy of this dataset. The datasets accept a transform callback that can preprocess each item. Here we cast data and label to floats and normalize data to range [0, 1]:\n",
    "\n",
    "## MNIST 数据集\n",
    "这一次我们将使用真实的数据，每一个28到28个中央剪裁的黑白数字的手写数字。我们的任务是拿出一个可以把每个图像与数字（0-9）关联在一起的模型。\n",
    "开始，我们将使用MXNet工具抓取一个数据集的副本。数据集接受一个可以对每个项目进行预处理的转换回调函数。在这里，我们将数据和标签转换成浮点数，并将数据正常化到范围[0, 1]："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\general/.mxnet/datasets/mnist\\train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading C:\\Users\\general/.mxnet/datasets/mnist\\train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading C:\\Users\\general/.mxnet/datasets/mnist\\t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading C:\\Users\\general/.mxnet/datasets/mnist\\t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "mnist_train = mx.gluon.data.vision.MNIST(train=True, transform=transform)\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts of the dataset for training and testing. Each part has N items and each item is a tuple of an image and a label:\n",
    "\n",
    "数据集有用于训练和测试的两个部分。每个部分有n个项，每个项是一个图像和一个标签的元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) 5.0\n"
     ]
    }
   ],
   "source": [
    "image, label = mnist_train[0]\n",
    "print(image.shape, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each image has been formatted as a 3-tuple (height, width, channel). For color images, the channel would have 3 dimensions (red, green and blue).\n",
    "\n",
    "请注意，每个图像被格式化为一个三元组（高度、宽度、通道）。对于彩色图像，通道将有3个维度（红色、绿色和蓝色）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the data and label shapes\n",
    "\n",
    "Generally, we don't want our model code to care too much about the exact shape of our input data. This way we could switch in a different dataset without changing the code that follows. Let's define variables to hold the number of inputs and outputs.\n",
    "\n",
    "\n",
    "## 记录数据和标签形状\n",
    "\n",
    "一般来说，我们不希望我们的模型代码过分关注输入数据的精确形状。通过这种方式，我们可以切换到不同的数据集，而不必更改下面的代码。让我们定义变量来保存输入和输出的数量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning libraries generally expect to find images in (batch, channel, height, width) format. However, most libraries for visualization prefer (height, width, channel). Let's transpose our image into the expected shape. In this case, matplotlib expects either (height, width) or (height, width, channel) with RGB channels, so let's broadcast our single channel to 3.\n",
    "\n",
    "\n",
    "机器学习库通常希望在（批处理、通道、高度、宽度）格式中找到图像。然而，大多数可视化的库都喜欢（高度、宽度、通道）。让我们把图像转换成期望的形状。在这种情况下，matplotlib要求（高度、宽度）或 带有RGB通道的（高度、宽度、通道），所以让我们广播单个渠道到3个渠道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "im = mx.nd.tile(image, (1,1,3))\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize our image and make sure that our data and labels line up.\n",
    "\n",
    "现在我们可以可视化我们的图像，确保我们的数据和标签排成一排。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe51f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's a beautiful five. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data iterator\n",
    "\n",
    "Now let's load these images into a data iterator so we don't have to do the heavy lifting. \n",
    "\n",
    "## 加载数据迭代器\n",
    "\n",
    "现在，让我们把这些图像加载到数据迭代器中，这样我们就不必做繁重的工作了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = mx.gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to want to load up an iterator with *test* data. After we train on the training dataset we're going to want to test our model on the test data. Otherwise, for all we know, our model could be doing something stupid (or treacherous?) like memorizing the training examples and regurgitating the labels on command.\n",
    "\n",
    "我们还想加载一个带有*test* 数据的迭代器。在训练数据集上训练之后，我们将要在测试数据上测试我们的模型。否则，我们所知道的，我们的模型可能是在做一些愚蠢的事情（或不可信的）？像记忆训练实例和照搬的标签命令。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = mx.gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate model parameters\n",
    "\n",
    "Now we're going to define our model. For this example, we're going to ignore the multimodal structure of our data and just flatten each image into a single 1D vector with 28x28 = 784 components. \n",
    "\n",
    "Because our task is multiclass classification, we want to assign a probability to each of the classes P(Y=c|X) given the input X. In order to do this we're going to need one vector of 784 weights for each class, connecting each feature to the corresponding output. Because there are 10 classes, we can collect these weights together in a 784 by 10 matrix.\n",
    "\n",
    "We'll also want to allocate one offset for each of the outputs. We call these offsets the *bias term* and collect them in the 10-dimensional array ``b``.\n",
    "\n",
    "## 分配模型参数\n",
    "\n",
    "现在我们来定义我们的模型。在这个例子中，我们会忽略我们多样的数据结构和展开每个图像到一个28x28 = 784成分单一的一维向量。\n",
    "\n",
    "因为我们的任务是多类分类，我们要分配一个概率给每个给定输入X类P（Y = C | x）。对于每个类，我们需要784个权重向量，将每个特征连接到相应的输出。因为有10个类，我们可以用784到10的矩阵来收集这些权重。\n",
    "\n",
    "我们还希望为每个输出分配一个偏移量。我们将这些偏移称为*偏置项*，并将它收集到10维数组中。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = nd.random_normal(shape=(num_inputs, num_outputs),ctx=ctx)\n",
    "b = nd.random_normal(shape=num_outputs,ctx=ctx)\n",
    "\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to let MXNet know that we'll be expecting gradients corresponding to each of these parameters during training.\n",
    "\n",
    "之前，在训练期间，我们需要让MXNet知道我们会期望梯度对应于这些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression\n",
    "\n",
    "In the linear regression tutorial, we performed regression, so we had just one output *yhat* and tried to push this value as close as possible to the true target *y*. Here, instead of regression, we are performing *classification*, where we want to assign each input *X* to one of *L* classes. \n",
    "\n",
    "The basic modeling idea is that we're going to linearly map our input *X* onto 10 different real valued outputs ``y_linear``. Then before, outputting these values, we'll want to normalize them so that they are non-negative and sum to 1. This normalization allows us to interpret the output yhat as a valid probability distribution.\n",
    "\n",
    "## 多类逻辑回归\n",
    "\n",
    "在线性回归的教程，我们进行了回归，所以我们只有一个输出*yhat*，试图让这个值尽量接近真实的目标*y*，在这里，不是回归，我们进行分类，我们要给每一个输入*X* 一个*L* 类。\n",
    "\n",
    "基本的建模思想是，我们将线性映射输入*X*到10个不同的实际输出值 ``y_linear``。然后，输出这些值，我们希望正则化它们，使它们为非负数，相加的和为1。正规化允许我们通过一个有效的概率分布来解释输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    norms = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.47811311  0.96285349  1.72885859  0.42453527  1.72286677 -0.32012343\n",
      "  -0.9782294   2.06755948 -1.71680093  0.47031024]\n",
      " [ 0.24345131  0.30026805  1.90792441  0.06960804 -0.29493463  1.2498771\n",
      "  -1.96911538 -1.7219516  -0.68004459 -0.33261022]]\n",
      "<NDArray 2x10 @cpu(0)>\n",
      "\n",
      "[[ 0.05805701  0.09427025  0.20278998  0.0550283   0.20157854  0.02613274\n",
      "   0.01353235  0.2845394   0.0064657   0.05760575]\n",
      " [ 0.07867803  0.08327769  0.41564727  0.06612326  0.04592355  0.21524774\n",
      "   0.0086089   0.01102274  0.03124527  0.04422554]]\n",
      "<NDArray 2x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "sample_y_linear = nd.random_normal(shape=(2,10))\n",
    "print(sample_y_linear)\n",
    "sample_yhat = softmax(sample_y_linear)\n",
    "print(sample_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that indeed all of our rows sum to 1. \n",
    "\n",
    "让我们确认所有的行总和确实为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1.  1.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(nd.sum(sample_yhat, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for small rounding errors, the function works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Now we're ready to define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    y_linear = nd.dot(X, W) + b\n",
    "    yhat = softmax(y_linear)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  cross-entropy loss function\n",
    "\n",
    "Before we can start training, we're going to need to define a loss function that makes sense when our prediction is a  probability distribution. \n",
    "\n",
    "The relevant loss function here is called cross-entropy and it may be the most common loss function you'll find in all of deep learning. That's because at the moment, classification problems tend to be far more abundant than regression problems. \n",
    "\n",
    "The basic idea is that we're going to take a target Y that has been formatted as a one-hot vector, meaning one value corresponding to the correct label is set to 1 and the others are set to 0, e.g. ``[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]``. \n",
    "\n",
    "\n",
    "The basic idea of cross-entropy loss is that we only care about how much probability the prediction assigned to the correct label. In other words, for true label 2, we only care about the component of yhat corresponding to 2. Cross-entropy attempts to maximize the log-likelihood given to the correct labels.\n",
    "\n",
    "\n",
    "## 交叉熵损失函数\n",
    "\n",
    "在我们开始训练之前，我们需要定义一个损失函数。 在我们的预测是一个概率分布的时候，\n",
    "这里的相关损失函数称为交叉熵，它可能是所有深度学习中最常见的损失函数。这是因为目前分类问题往往比回归问题丰富得多。\n",
    "\n",
    "基本思想是将目标格式化为一个热向量，意思是对应于正确标签的一个值设置为1，其他值设置为0，例如[ 0, 1, 0，0, 0, 0，0, 0, 0，0 ]。\n",
    "交叉熵损失的基本思想是，我们只关心分配给正确标签的预测概率。换句话说，对于真正的标签2，我们只关心2相应的组件。交叉熵试图最大限度地提高正确标签的对数似然。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.sum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "For this example we'll be using the same stochastic gradient descent (SGD) optimizer as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluation loop to calculate accuracy\n",
    "\n",
    "While cross-entropy is nice, differentiable loss function, it's not the way humans usually evaluate performance on multiple choice tasks. More commonly we look at accuracy, the number of correct answers divided by the total number of questions. Let's write an evaluation loop that will take a data iterator and a network, returning the model's accuracy  averaged over the entire dataset.\n",
    "\n",
    "## 写求值循环计算准确度\n",
    "\n",
    "虽然交叉熵是好的，可微的损失函数，但这并不是人类通常在多项选择任务上评估性能的方法。更常见的是我们看准确度，正确答案的数量除以问题总数。让我们编写一个包含数据迭代器和网络的求值循环，返回模型在整个数据集上的平均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we initialized our model randomly, and because roughly one tenth of all examples belong to each of the ten classes, we should have an accuracy in the ball park of .10.\n",
    "\n",
    "因为我们随机初始化了我们的模型，因为大约十分之一的例子都属于这十个类中的每一个，所以我们应该在10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16329999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.34971372768, Train_acc 0.754017, Test_acc 0.7633\n",
      "Epoch 1. Loss: 0.899625499827, Train_acc 0.813233, Test_acc 0.8251\n",
      "Epoch 2. Loss: 0.811315553979, Train_acc 0.836983, Test_acc 0.8499\n",
      "Epoch 3. Loss: 0.723591087432, Train_acc 0.850333, Test_acc 0.8618\n",
      "Epoch 4. Loss: 0.642600380623, Train_acc 0.859183, Test_acc 0.8684\n",
      "Epoch 5. Loss: 0.609362499913, Train_acc 0.866733, Test_acc 0.8745\n",
      "Epoch 6. Loss: 0.585710662143, Train_acc 0.871617, Test_acc 0.8778\n",
      "Epoch 7. Loss: 0.545416593207, Train_acc 0.875167, Test_acc 0.8817\n",
      "Epoch 8. Loss: 0.508563420335, Train_acc 0.878633, Test_acc 0.8848\n",
      "Epoch 9. Loss: 0.492235264934, Train_acc 0.881983, Test_acc 0.8874\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "smoothing_constant = .01\n",
    "niter=0\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        niter +=1\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + (smoothing_constant) * nd.mean(loss).asscalar()\n",
    "        est_loss = moving_loss/(1-(1-smoothing_constant)**niter)\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, est_loss, train_accuracy, test_accuracy))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for prediction\n",
    "Let's make it more intuitive by picking 10 random data points from the test set and use the trained model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABECAYAAACRbs5KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138827f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions are: \n",
      "[ 6.  1.  8.  4.  5.  1.  8.  4.  0.  9.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# Define the function to do prediction\n",
    "def model_predict(net,data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, 10, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(ctx)\n",
    "    print(data.shape)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Jeepers. We can get nearly 90% accuracy at this task just by training a linear model for a few seconds! You might reasonably conclude that this problem is too easy to be taken seriously by experts.\n",
    "\n",
    "But until recently, many papers (Google Scholar says 13,800) were published using results obtained on this data. Even this year, I reviewed a paper whose primary achievement was an (imagined) improvement in performance. While MNIST can be a nice toy dataset for testing new ideas, we don't recommend writing papers with it. \n",
    "\n",
    "\n",
    "结论\n",
    "天啊。我们只需训练一个线性模型几秒钟就能得到接近90%的精确度！你可以合理地得出这样的结论：这个问题很容易被专家们认真对待。\n",
    "但直到最近，许多论文（谷歌学者说13800）都是根据这个数据获得的结果发表的。甚至在今年，我还回顾了一篇论文，其主要成就是（想象中）业绩的提高。在MNIST可以测试新的想法，一个好玩具的数据集，我们不推荐用它写论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Softmax regression with gluon](../chapter02_supervised-learning/softmax-regression-gluon.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
